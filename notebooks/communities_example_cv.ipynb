{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communities and Crime Cross-Validation\n",
    "Data is taken from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime).\n",
    "> U. S. Department of Commerce, Bureau of the Census, Census Of Population And Housing 1990 United States: Summary Tape File 1a & 3a (Computer Files),\n",
    ">\n",
    "> U.S. Department Of Commerce, Bureau Of The Census Producer, Washington, DC and Inter-university Consortium for Political and Social Research Ann Arbor, Michigan. (1992)\n",
    ">\n",
    "> U.S. Department of Justice, Bureau of Justice Statistics, Law Enforcement Management And Administrative Statistics (Computer File) U.S. Department Of Commerce, Bureau Of The Census Producer, Washington, DC and Inter-university Consortium for Political and Social Research Ann Arbor, Michigan. (1992)\n",
    ">\n",
    "> U.S. Department of Justice, Federal Bureau of Investigation, Crime in the United States (Computer File) (1995)\n",
    ">\n",
    "> Redmond, M. A. and A. Baveja: A Data-Driven Software Tool for Enabling Cooperative Information Sharing Among Police Departments. European Journal of Operational Research 141 (2002) 660-678.\n",
    "\n",
    "*This notebook compares the performance of warped linear regression to ordinary least squares on a cross-validation of the data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing(df):\n",
    "    drop_columns = []\n",
    "    for column in df.columns:\n",
    "        if '?' in list(df[column].values):\n",
    "            drop_columns.append(column)\n",
    "    return df.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    df = df.drop(range(5), axis=1)\n",
    "    df = drop_missing(df)\n",
    "    X = np.array(df.iloc[:,:-1].values, dtype=float)\n",
    "    y = np.array(df.iloc[:,-1].values, dtype=float)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset(\"communities.data\")\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Cross-Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_splits = list(KFold(10, shuffle=True).split(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Warping Parameters\n",
    "The warping parameters were fit to maximize the log-likelihood of the training data for each cross-validation using a second order optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warping_parameters = np.array([\n",
    "    [29.11410629,  8.2121922 ,  0.09806633],\n",
    "    [29.5381302 ,  8.18131267,  0.09972036],\n",
    "    [31.86772755,  8.33747215,  0.09740263],\n",
    "    [33.0122389 ,  8.22343264,  0.09973035],\n",
    "    [28.2135524 ,  8.24779062,  0.09725041],\n",
    "    [29.95950517,  8.11544552,  0.10044195],\n",
    "    [25.12550611,  8.34099059,  0.09358942],\n",
    "    [-12.96473319, 8.17310442,   0.0867982],\n",
    "    [29.97945979,  8.16349394,  0.09975204],\n",
    "    [35.60438079,  8.34048291,  0.09914208]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorator for Vector Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizable(f):\n",
    "    def f_(x):\n",
    "        if hasattr(x, '__iter__'):\n",
    "            return np.array([f(xi) for xi in x])\n",
    "        return f(x)\n",
    "    return f_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Warping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_warper(parameters, y):\n",
    "    y_mean = np.mean(y)\n",
    "    y_std = np.std(y)\n",
    "    def f(t):\n",
    "        t = (t - y_mean) / (y_std * np.sqrt(len(y)))\n",
    "        result = t\n",
    "        for i in range(0, len(parameters), 3):\n",
    "            a, b, c = parameters[i:(i+3)]\n",
    "            result += a**2*math.tanh(b**2*(t + c))\n",
    "        return result\n",
    "    mean = np.mean([f(yi) for yi in y])\n",
    "    @vectorizable\n",
    "    def f_(t):\n",
    "        return f(t) - mean\n",
    "    @vectorizable\n",
    "    def fp(t):\n",
    "        t = (t - y_mean) / (y_std * np.sqrt(len(y)))\n",
    "        result = 1.0\n",
    "        for i in range(0, len(parameters), 3):\n",
    "            a, b, c = parameters[i:(i+3)]\n",
    "            u = math.tanh(b**2*(t + c))\n",
    "            result += a**2*b**2*(1 - u**2)\n",
    "        result /= y_std * np.sqrt(len(y))\n",
    "        return result\n",
    "    return f_, fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Log-Likelihood Proxy of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood_proxy(X, y, phi):\n",
    "    f, fp = make_warper(phi, y)\n",
    "    z = f(y)\n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X, z)\n",
    "    z_pred = model.predict(X)\n",
    "    rss = sum((z-z_pred)**2)\n",
    "    return -len(y)/2*np.log(rss) + sum(np.log(fp(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Maximums\n",
    "Tweak the warping parameters to verify they're a local maximum of the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_log_likelihood_opt(X, y, phi):\n",
    "    delta_x = 1.0e-3\n",
    "    for i, phi_i in enumerate(phi):\n",
    "        def f(x):\n",
    "            phi_copy = np.array(phi)\n",
    "            phi_copy[i] = x\n",
    "            return compute_log_likelihood_proxy(X, y, phi_copy)\n",
    "        f0 = f(phi_i)\n",
    "        for x in [phi_i - delta_x, phi_i + delta_x]:\n",
    "            delta_f = f(x) - f0\n",
    "            relative_delta_f = delta_f / delta_x\n",
    "            if relative_delta_f > 0 and np.abs(relative_delta_f) > 1.0e-3:\n",
    "                print(i, x, \"\\t\", delta_f, relative_delta_f)\n",
    "                assert False, \"Can't verify optimum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0 29.5371302 \t 2.3903871806396637e-06 0.0023903871806396637\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Can't verify optimum",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-33ac7ccf994c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mverify_log_likelihood_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarping_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-6c2a1cbf3760>\u001b[0m in \u001b[0;36mverify_log_likelihood_opt\u001b[0;34m(X, y, phi)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrelative_delta_f\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_delta_f\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0e-3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_delta_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Can't verify optimum\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Can't verify optimum"
     ]
    }
   ],
   "source": [
    "for index, (train_indexes, _) in enumerate(cv_splits):\n",
    "    print(index)\n",
    "    X_train = X[train_indexes, :]\n",
    "    y_train = y[train_indexes]\n",
    "    verify_log_likelihood_opt(X_train, y_train, warping_parameters[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
